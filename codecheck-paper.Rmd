---
title: "CODE CHECK"
author:
- affiliation: Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, Cambridgeshire, GB
  corresponding: sje30@cam.ac.uk
  email: sje30@cam.ac.uk
  name: Stephen Eglen
  orcid: 0000-0001-8607-8025
- affiliation: Institute for Geoinformatics, University of M"unster, M"unster, Germany
  email: daniel.nuest@uni-muenster.de
  name: Daniel Nüst
  orcid: 0000-0002-0024-5046
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: |
          | ...
#author_summary: |
#  TBD
bibliography: bibliography.bib
#output: rticles::plos_article
#csl: plos.csl
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- Let's stick to one sentence per line! -->
<!-- Citations: @Figueredo:2009dg, citep: [@Figueredo:2009dg], multiple citep: [@Figueredo:2009dg; @other_2018] -->
<!-- add " {-}" after headings to remove numbering -->


# Open questions

* Prefer longlines or short?
* Can we switch to latex for this?  (Find it slightly easier.)
* Which journal to target -- maybe worry abotu that once we have a
  preprint.  F1000R my current favourite?
  
* Check with Giuliano re: "code works".


# Introduction

Many areas of scientific research now use computation to either simulate or analyse their data.  These computations are increasingly complex, and difficult to explain coherently in a paper. (MARWICK).  To complement the traditional route of sharing research by writing papers, there is a growing trend/demand to share the underlying artifacts, notably code and datasets, so that others can inspect, reproduce or expand that work (Figure 1).  Some of the earliest proponents were Buckheit and Donoho (1995) who commented "advert is scholarship for the paper". (get correct quote).

So, assuming that researchers begin to share more artifacts, how
might they be examined to test that they do what they claim?  For
example, most scientific journals now require a data sharing statement
that outlines what data the authors have (or will) share.  The
implementation status of this varies considerably according to the
journal.  At one of the spectrum, there are specialist journals  that
have been created to accept "data papers" (e.g. *Scientific Data*,
... NAME COUPLE OF OTHERS).  These journals have established rigorous
procedures by which the data is validated according to standards in
each field.  At the other end of the spectrum, we still witness today
the infamous statement "Data available upon reasonable request"
whereby authors, well-intentioned, often cannot provide the data when
readers ask for it.

Given that there are no clear standards yet for sharing data, what
hope might there be for sharing computer programs?  Our anectodal
experiences on this matter suggest that often a variety of reasons are
given for why their code cannot be shared, e.g. "there is no
documentation / I do not want to maintain it / I do not want to give
away my code to my competitors".  Our view, rooted in open science
princples, is that wherever possible sharing code is good for the
community, as outlined ten years ago (Barnes, 2010).  Having the code
freely available, and suitably archived, provides a valuable resource
for others to learn from, even when it doesn't run or if there is no
documentation.  However, with a little effort, we believe that if
someone independent can run/use the code, this is worth documenting as
early as possible.  With this in mind, we have developed a set of
PRINCIPLES and an example workflow that provides what he hope is a
pragmatic way of checking that code works.  We call this system
CODECHECK as outlined next.


\begin{figure}
  \centering
  \includegraphics{figs/rr.pdf}
  \caption{The inverse problem in reproducible research.  The left
  half of the diagram shows the diverse range of materials used
  internally within a laboratory.  These materials are often then
  condensed into a paper for sharing with the outside world via the
  research paper, astatic PDF document.  Working backwards from the
  PDF to the underlying materials is impossible.  By sharing the
  materials on the left, others outside the lab are able to reproduce
  or build on this work.  (Should we draw a backward arrow showing the
  impossibility of this backward flow?)}
  \label{fig:inverse}
\end{figure}




# What is a CODECHECK?

CODECHECK is best demonstrated by way of our example workflow, and
later we expand on the underlying principles.  The workflow involves
three groups of people (1) the AUTHOR providing the code to be
checked, (2) the PUBLISHER of a journal intereseted in publishing the
paper by the AUTHOR and (3) the CODECHECKER who checks that the
AUTHOR's code works.  The workflow that we have refined is documented
in Figure 2.

Step 1: the author submit their manuscript along with code and data
to the publishers.  The code and data need not be openly available at
this point.

Step 2: the publisher finds a codechecker to check the code.  This is
analgous to the publisher finding one or more reviewers to evaluating
the code.  However a cruical difference is that we suggest the
codechcker and the author can talk freely to each directly (see next).

Step 3: the codechecker runs the code, based on instructions provided
by the author, and checks if some/all of the results from the paper
can be reproduced.  (See later for exact nature of "reproduced").  If
there are any problems running the code, the codechecker asks the
author for help to resolve the problems.  The codechecker then tries
to run the code again.  This process iterates until either the
codechecker is successful, or the codechecker concludes the code does
not (*what do in this case*).  As part of this process, the
codechecker could work entirely locally on their own compute resource,
or in the cloud, e.g. using mybinder infrastructure.  Using such
cloud-based infrastructure allows for the codechecker and author to
collaboratively run the code together.

Step 4: The codechecker writes a certificate stating how the code was
run and includes a copy of outputs (figures, tables) that were
independently generated.

Step 5: the certificate and code/data get deposited on an open archive
(currently Zenodo), and the certificate is given to the PUBLISHER.

Step 6: the Publisher can then use the certificate as part of their
supplementary records for a paper, and give credit to the codechecker
for their work by depositing appropriate records (*bit hazy here*).


\begin{figure}
  \centering
  \caption{The detective. Could we number the arrows as steps?
(Implementing the CODECHECK process).  DANIEL: best source for this figure?}
  \label{fig:worfklow}
\end{figure}



## Variations

This example workflow leaves sufficient room for different
implementations of this process.  There are several aspects that can
be considered:

### When to do a codecheck

preprint (no publisher available), pre-condition for peer review,
during peer review, after acceptance.  Each of these have their own
pros/cons.

### Who does the codecheck

in house staff, pool of ECRs, volunteeres,

### Who knows who?

single blind, double blind, private/public communication between author and
codechecker, collaborative.  (This might need finessing with principle
2 below).

## Core principles

The workflow and variations just outlined below reflect our current
views on how a codecheck should be performed.  They are not set in
stone, but we do believe the following four core principles underlying
CODECHECK:

**1. Codecheckers record but don’t investigate or fix.**  The
codechecker follows the instructions provided by the author in running
the code.  If any steps are unclear, or if code does not run
correctly, the codechecker records this fact, and then communicates
this information to the author.  We believe that the job of the
codechecker is not to fix all these problems, but simply to report
them, and await a fix from the author.

**2. Communication between humans is key.**  Some code may just work
without any interaction (e.g. Manuel's certificate); however, more
often there are hidden dependencies that need to be found, or revised
software installations.  Allowing the codechecker and human to
communicate directly and openly with each other should make this
process as constructive as possible.  We feel that routing this
conversation (possibly anonymously) through a publisher would slow
things down and reduce the chances for community bulding.

**3. Credit is given to codecheckers.** (TODO Explain in a few
sentences what each means, e.g. see 1,2)

**4. Workflows must be auditable.** (TODO Explain in a few sentences
what each means, e.g. see 1,2)


# Register

We have a curated list of 15 certificates available at
<https://codecheck.org.uk/register>.  These are a mixture of
reproductions of papers for a variety of reasons (do we name them
all?) from interesting historical papers, to papers codechecked as
part of peer review, or because of public interest (covid).  several
covid models have been done during the time of peer review of these
papers, under our initative rather than requested from journal, but
certificate have then been acknowledged/cited in the paper (LANCET /
LSHTM).


We would like to briefly highlight three examples here.

1. Gigascience -- our first paper.  Only visualisation, not machine
   learning (which would have taken sseveral days of compute time).
2. report 9 -- covid model of UK lockdown model.
3. Geoscience

## Annotated certificate

Should we annotate a certificate, at least the cover page, to
demonstrate what we mean by a certificate?  We could print a four page
certificate 4-up so that it fits onto one page.  Text might be too
small though to read?  A 2-up, printed sideways would work well
though.

To run the system, we rely on github to host our work, and use a
custom R pacakge, codecheck, to help with the generation of
certificates and depositing work on zenodo.  (Cite zen4R).

# Related work

All the related projectsfrom A-Z... e.g. CASCAID, Rescience, modeldb,
o2r.  Community iniatives like reprohack along similar lines of
checking code. <https://reprohack.github.io/reprohack-hq/>


# Limitations

Handling failure:  we have no established process for this, as so far
all our codechecks succeeded.  Open question remains as what to do in
this case -- should we publicly report that the code would not run.

Compute time:  for those papers that take significant compute time
(think days, not minutes), who will pay for the compute time?

Software: authors can currently provide code that requires
propieratiry software.  Given the prevalence of software like MATLAB,
this seems like a pragmatic choice, but then (a) we can't rely on
open infrastructure like mybinder, (b) codechecker must have that
software.

Can't someone cheat? yes.  we don't check it is correct code, just
simply that it runs.  Very low bar.  But by having the code open,
interested parties can then examine the code, and knowing that the
code will be open we hope will be an incentive for authors to share.

Who's got time for more peer review?  Agree, but ECRS might be
attracted to working with groups (recall it is not anonymous)

Who finds the codechecker?  Good point!  The journal.

As a principle, we do not require results to be exactly the same for a
codecheck to pass, simply that the code runs and generates the output
files that the author claims.  Stochastic simulations mean that often
we cannot get exactly the same, or even different versions of
libraries.  (Cite freesurfer example that generates different outputs
on different operating systems).

- modelDB principle: minimum of one figure should be reproducibe.
  Exampler certificate 2020-016 (I think) which had only a small
  subset of the possible figures.  Still useful and worth sharing.


# Future work and conclusions

- possibility to allow CI for open papers such that for "nearly free"
  we can find out when softwave breaks, and if provenance tools are
  used, when.
  
- How to embed this in juornals.
- ???



- role of data and code, data science
- reproducible resarch
- data enclaves
- CASCAID
- ROpenSci/PyOpenSci (and how their software review differs, though they also collaborate with journals, https://devguide.ropensci.org/softwarereviewintro.html)
- time capsules > https://twitter.com/DougBlank/status/1135904909663068165?s=09
- science is changing: https://theconversation.com/what-you-need-to-know-about-how-coronavirus-is-changing-science-137641
- reproducibility is important, also
  - effects on well-being/mental health, https://www.nature.com/articles/d41586-020-01642-9

**main contributions**

- a concept for integrating a minimal code review into common scholarly communication processes around peer review
- set of shared principles (recognition value) that allow researchers to quickly understand the level of evaluation across journals/publishers/conferences, and allows helps these stakeholders to establish workflow checking in their domain/solution/product
- a common language and rubric for classifying different levels of code and data evaluation as part of a peer-review

# What is CODE CHECK?

- **Principles**: https://codecheck.org.uk/
- relation to "proper" citation of data and software, and depositing of data and software in suitable repos _besides_ the bundle for CODE CHECK: do it, only provide the concrete analysis script for the check
- **Why is it useful?**
  - reduce the barrier to evaluating non-text parts of research
  - acknowledges different skill sets and existing high load on reviewers
  - CC breaks problem that publishers staff expertise today is not likely to suffice
  - engaging ECRs is a perfet fit (they are up-to-date in methods, are introduced to peer review)
  - relation to research compendia? point it out for authors unsure about how to structure their workflow
  - CrossRef and Publons and ORCID have features around reviews
  - shifting burden from reviewer to author (but not too much)
  - What issues with (open) peer review does CODECHECK address?
- keynote https://doi.org/10.7557/5.4963: in discussion, the speaker mentioned value of peer review for "baseline reassurance", i.e. a paper has at least been checked by someone with an understanding, to increase trust especially if looking at papers from other disciplines
- **report** must fulfil some requirements to make clear what is really checked and not mislead - readers will have very different understandings of a green checkmark!
- reproducibility is hard https://twitter.com/annakrystalli/status/1144176149859377152?ref_src=twsrc%5Etfw > code check gives at least some benefits
- what are the requirements / level or reproducibility required or checked?
  - [Stodden et al., 2013](https://stodden.net/icerm_report.pdf): five-level hierarchy of research - reviewable, replicable, confirmable, auditable, and open or reproducible
  - [Whitaker 2017](https://figshare.com/articles/Showing_your_working_A_guide_to_reproducible_neuroimaging_analyses/4244996/2): reproducible, generalisable, replicability, robustnes
- can authors retract their code/data once a CODECHECK has started? In the eLife prepreint review, there seems no "way back", see https://twitter.com/ceptional/status/1271528976763121664?s=09

# Related work

- CODECHECK vs. Verification Reports > > https://www.sciencedirect.com/science/article/pii/S0010945220301738#fn3
- CODECHECK vs. the "Artifacts Evaluated - Functional" badge of the ACM, see https://www.acm.org/publications/policies/artifact-review-badging
- Does CODECHECK do internal replication, as defined by https://gatesopenresearch.org/articles/4-17/v2 ?
- https://paperpile.com/shared/rVNwBS
- Are "Replicated Computations Results (RCR) Report" a thing? apparently so (using ACM terminology)
  - https://dl.acm.org/doi/10.1145/3185337
  - http://users.iems.northwestern.edu/~nelsonb/Publications/Nelson_RCRgreenSim.pdf
  - https://dl.acm.org/doi/epdf/10.1145/3341094
    - citation for replicated workflow lists it as "to appear"
  - getting proper papers for reproductions != CODECHECK approach where we want credit as reviewers
  - see also [Google Search](https://www.google.com/search?q=%22Replicated+Computations+Results+%28RCR%29+Report+for%22)
- https://medium.com/bits-and-behavior/a-modern-vision-for-peer-review-d5f73f0fae07 - work by Amy J. Ko: https://faculty.washington.edu/ajko/publications
  - is CODE CHECK's process too small, too incremental a change?
- overview of peer review and bibliometrics (if not a reference itself, it gives plenty of references): http://juser.fz-juelich.de/record/865096/files/Peer%20Review%20and%20Bibliometrics.pdf
- Open Peer Review > https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-019-0063-9
  - Are OPR lessons transferable to CODECHECK? Does CODECHECK work better with OPR than with (single/double) blind peer review?
  - similarities to their advice: open reports, give credit
- _Successes and struggles with computational reproducibility: Lessons from the Fragile Families Challenge_ > https://osf.io/preprints/socarxiv/g3pdb/
- community standards for software, cf. slide 6 in https://figshare.com/articles/_/5675104
  - "good enough practice" is "get a colleague to try using it", slide 19 in https://figshare.com/articles/_/5675104
- journals doing code reviews
  - _"We know of 16 journals that take this valuable step (ie TOP Level 3): AEA as already mentioned, plus others in polisci, psychology, biostats, and org chem: see Data Rep Policies here https://t.co/plOF8j6ADU "_ (https://twitter.com/EvoMellor/status/1202692360456589339?s=09); see also whole thread!
  - https://medium.com/@NeurIPSConf/call-for-papers-689294418f43
  - https://www.journals.elsevier.com/information-systems/editorial-board/
  - SIGMOD Reproducibility Review?
  - Biostatistics "AER" (https://academic.oup.com/biostatistics/article/10/3/405/293660)
- open science platforms/cloud services > https://www.nature.com/articles/d41586-019-03366-x
- live code in articles > o2r, eLife RDS https://www.nature.com/articles/d41586-019-00724-7, ...
- Artefact Evaluation - https://www.artifact-eval.org/about.html
- continuous analysis - https://www.nature.com/articles/nbt.3780
- "A Universal Identifier for Computational Results" > https://www.sciencedirect.com/science/article/pii/S1877050911001256
- Discussion about using Docker containers during JOSS reviews > https://github.com/openjournals/joss/issues/498#issuecomment-462046912
- https://scigen.report, an independent site for registering reproductions, via https://annakrystalli.me/talks/ro-reprohack.html#32
  - example https://scigen.report/reviews/get?doi=10.1063/1.1823034
  - seems to be closed infrastructure
- new journal _ReScienceX_ (about experiments) also seems to have an ECR angle, https://www.nature.com/articles/d41586-020-01328-2
- really interesting work on reproductions in hydrology, with a neat survey that reproducers filled out which might be interesting to capture from codecheckers, too: https://www.nature.com/articles/sdata201930#Sec9

# CODE CHECK implementation concepts

- need to get wording straight: process/implementation/workflow (DN: suggest to use "workflow" for research-analysis workflow)
- https://codecheck.org.uk/process/
- _dimensions_ along which a check can vary
- independent/pre-submission (Peer_Coding) by author + colleagues
- AGILE
- premier (OA) publisher
- community OA / scholarly society with public peer review
- invited reproducibility paper track, cf. https://www.elsevier.com/journals/information-systems/0306-4379/guide-for-authors
- on giving credit: public databases
  - https://www.reviewercredits.com/
  - Publons
  - ORCID
- compare with existing processes that are publicly documented
  - AJPS Verification Policy: https://ajps.org/ajps-verification-policy/
  - ...
- flexible level of detail? a check can be enhanced to (potentially semi-automatically) check for good practices
  - "A group of us in the @force11rescomm Software Citation Implementation Working Group produced a "Software Citation Checklist for Authors" https://t.co/4Noe66FsoX providing guidelines on how to cite software and describing why it is important and what software should be cited" via https://twitter.com/alegonbel/status/1207664922932453376?s=09)
- focus on ECR as codecheckers
  - ECR are the one's breaking new ground in schol comm, cf. the percentages of career stages submitting registered reports on slide 20 > https://osf.io/2vjus/ 

# Annotated Examples
<!-- see examples.md -->

- **the default implementation**
- critically discuss our/the default implementation vs. the principles
- HPC example
- if we see the corpus of examles as a scientific dataset, it should be published in a citable way (maybe even submit to a data journal)

# Open problems

- Do publishers need to build up data science experts who can conduct checks?
- Is an integration of software/data citation checks benefitial/realistic?
- Bot-support and tools (linter, report authoring with R Markdown)
- How can open platforms be used to enable a review in private (cf. https://www.cambridge.org/core/blog/2019/08/19/how-to-make-the-data-and-code-for-your-manuscript-available-to-peer-reviewers-before-making-it-public/), if that is desired
- Does nudging towards better practices work (highlight the successes) or is any check with opt-in doomed to fail?
- Do we need more than one codechecker? Is there a similar diversity as in a peer review where more opinions are needed?
- can CODECHECK work for preprints? how? is a re-check required?
- Risk: is the hurdle too low? are "levels" of checks are solution to increase transparency of check's level of detail
- CODECHECK in the context of innovations and disruptions in scientific publishing
  - https://doi.org/10.1042/ETLS20180172
  - https://doi.org/10.3390/publications7020034
- Should every code be checkd, or should the labour be capped by only checking "important" works or at important stages; if yes, isn't peer review such a stage?
- see https://github.com/codecheckers/discussion
- Can we establish code checks as a type of review within the schol comm metadata (or should we?) > we should not, put them on the same level as reviews!
- How can we turn a CODECHECK into a positive experience?
  - cf. kindness in peer review in https://www.mdpi.com/2304-6775/8/2/26
- How can we allow reactions to checks? (commenting on reports)
- Will bitwise reproducibility become easier than what a CODECHECK does? triggered by https://twitter.com/khinsen/status/1242842759733665799?s=09)
- If a report is not published for failures: Isn't there a bias for the codechecker to have a "successful" reproduction because then a report is published?

# Declaration of interest

SJE is on the senior editorial board at the journal *Scientific Data*.

# Acknowledgements

Collaborators and potential collaborators:

We are grateful to the following individuals for discussions regarding
the work presented here: Andy Collings, Melissa Harrison, Giuliano
Maciocci (eLife), Rebecca Kirk (PLOS Computational Biology), Scott
Edmunds (Gigascience) Andrew Hufton (Scientific Data). Iain Davies and
Yuhao (Sebastian) Wang worked on developing code and example
certificates.  This work was financially supported by the UK Software
Sustainability Institute and a Mozilla Science mini grant.


# Contribution

SE ...; DN ...; see [CRediT](https://casrai.org/credit/).
