
Reviewer 2
<https://f1000research.com/articles/10-253/v1#referee-response-82472>

This paper outlines a set of principles and a community of practice
for verifying computational analyses can be run and research artefacts
reproduced as part of, or in addition to, traditional peer review
processes. The ongoing scientific reproducibility crisis and current
lack of many (or any) standards for checking computational research in
the publishing industry makes this an important, new framework to
share with the community.

The authors demonstrate a deep and thoughtful knowledge of the
cultural barriers surrounding such technological checks for peer
review, such as time, expertise, and bitwise comparative
reproducibility. They acknowledge that the specific incarnation of the
CODECHECK practice outlined in this paper is limited to provide a low
barrier for entry in order to encourage adoption, but do detail the
scope in which such a workflow could be adapted and built upon to
raise that bar and perform more stringent checks. Specifically, the
principles are not technology-based to allow for flexibility in the
complexity and domain of computational research to be checked. I
particularly appreciated the authors’ recommendation/suggestion that
CODECHECKs become a platform for engaging Early Career Researchers in
the peer review process.

Alongside CODECHECK’s own workflows (which are openly published on
GitHub and Zenodo), the paper outlines many similar and related
initiatives that fall within the CODECHECK framework providing a
wealth of examples for the community to draw inspiration from when
designing and applying their own CODECHECK workflows.


Is the rationale for developing a new method clearly explained?

The authors show a deep knowledge of the pitfalls of traditional peer
review of static research artefacts and clearly identify and outline
the rationale for a peer review-like system capable of assessing
computation-based research.


Is the description of the method technically sound?

I’m going to answer a slightly different question of “Is the
description of the method culturally sound?” This is because the
authors have intentionally not provided a technological methodology
for completing a CODECHECK so as to avoid vendor lock-in (e.g. cloud
platform providers) and to provide flexibility for applying the
methodology to a range of computational research domains. Instead, the
focus of the methodology is on building a community of practice around
having code mechanically checked by someone with comparable technical
expertise from outside the project.

The authors demonstrate a considerate knowledge of the burden of
verifying computational reproducibility on both authors and peer
reviewers and aim, not to increase this burden, but to provide an
entry point into a world where checking research code can be run and
produces the artefacts as they are presented in the paper is
normalised. I think their recommended approach focussing on
communication between codecheckers and authors, codecheckers will
check and not fix, and codecheckers being an additional role to the
traditional peer reviewer will aid early adoption of this framework.


Are sufficient details provided to allow replication of the method
development and its use by others?

The concept of CODECHECK is intentionally presented as a set of
principles and example workflows, as opposed to fixed, step-by-step
actions, to allow for flexibility across computational complexity and
research domains. The principles, example workflow, and potential
variations under this framework are explained in depth and examples of
workflows that fall under the CODECHECK framework from other
publishers and/or conferences are provided, alongside CODECHECK’s own
community. From this wealth of detail, I believe that others would be
able to replicate, adapt and apply a CODECHECK-like workflow in their
journal or community.


Are the conclusions about the method and its performance adequately
supported by the findings presented in this article?

It is encouraging to see that the community feedback from authors and
publishers shaped the workflow and principles that uphold CODECHECK
and a number of certificates have already been issued under this
framework. This shows that the workflow of a CODECHECK as outlined in
the paper is achievable in partnership with current peer review
operations. However, I would like to see the impact of the CODECHECK
certificates issued.

**1. Is there any community feedback on the transparency and
reusability of research published with CODECHECK certificates?**

This is perhaps too big of an ask this early in the
initiative as research reuse and citations are independent factors of
the publication and peer review of this specific paper - but I’d still
be interested in any insights the authors have to offer on this topic.

Is the rationale for developing the new method (or application) clearly explained?

Yes

Is the description of the method technically sound?

Yes

Are sufficient details provided to allow replication of the method development and its use by others?

Yes

If any results are presented, are all the source data underlying the results available to ensure full reproducibility?

No source data required

Are the conclusions about the method and its performance adequately supported by the findings presented in the article?

Partly
