\documentclass[12pt]{article}
\usepackage{f1000_styles}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{url}
\usepackage{setspace}

\usepackage{booktabs}
\usepackage{doi} % turn DOIs into links
\usepackage{nameref}

\onehalfspacing

\begin{document}
\title{CODECHECK: An open-science initiative to facilitate sharing of
  computer programs and results presented in scientific publications.}
\author[1,$\ast$]{Daniel N\"{u}st}
\author[2,$\ast$]{Stephen J Eglen}
\affil[1]{\url{https://orcid.org/0000-0002-0024-5046}}
\affil[2]{\url{https://orcid.org/0000-0001-8607-8025}}
\affil[$\ast$]{Contributed equally}
\affil[$$]{Public project for this article is at \url{https://github.com/codecheckers/paper}}
\maketitle
\begin{abstract}
\ldots{}
\end{abstract}

\section*{TODO}\label{todo}

\begin{itemize}
\item Check with Giuliano re: ``code works''. Write to Naomi.
\item Use "execution" insteand of "sharing" in title?
\item Can we introduce "research compendium" as a term?
\item Should we make code and data sharing a prerequisite or leave that door open?
\end{itemize}

\section*{Introduction}\label{introduction}

Many areas of scientific research now use computation to either simulate
or analyse their data. These computations are increasingly complex, and
difficult to explain coherently in a paper \citep{marwick_how_2015}.
To complement the traditional route of sharing research by writing papers,
there is a growing trend/demand to share the underlying artifacts, notably 
code and datasets, so that others can inspect, reproduce or expand that work
(see Figure~\ref{fig:inverse}).
Some of the earliest proponents were Buckheit and Donoho
\cite{buckheit_wavelab_1995} who coined what has been called 
\emph{Claerbout's claim} (extending on \citet{claerbout_electronic_1992}):
\emph{"An article about computational science in a scientific publication 
is \text{not} the scholarship itself, it is merely \textbf{advertising} of
the scholarship. The actual scholarship is the complete software development
environment and the complete set of instructions which generated the 
figures."}
%  Donoho (2010) has another paraphrasing of the same quote, which might work better:
% "an article about computational result is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result."

So, assuming that researchers begin to share more artifacts, how might
they be examined to test that they do what they claim? For example,
most scientific journals now require a data sharing statement that
outlines what data the authors have (or will) share.
The implementation status of this varies considerably according to the
journal. At one of the spectrum, there are specialist journals that
have been created to accept ``data papers'' (e.g.~\href{}{\emph{Scientific
  Data}}, 
\href{https://essd.copernicus.org/}{\emph{Earth System Science Data}}, 
\href{https://rmets.onlinelibrary.wiley.com/journal/20496060}{\emph{Geoscience Data Journal}},
\href{https://bdj.pensoft.net/}{\emph{Biodiversity Data Journal}},
\href{https://openpsychologydata.metajnl.com/}{\emph{Journal of Open Psychology Data}},
\href{https://odjar.org/}{\emph{Open Data Journal for Agricultural Research}},
\href{https://openhealthdata.metajnl.com}{\emph{Journal of Open Health Data}}).
These journals have established rigorous procedures by which
the data is validated according to standards in each field. At the
other end of the spectrum, we still witness today the infamous
statement ``Data available upon reasonable request'' whereby authors,
while possible well-intentioned at the time of writing the article,
often cannot provide the data when readers ask for
it.  (ANDREW H: any references to sum up poor/evolving state of data
sharing; Vines et al 2014).

Given that there are no clear standards yet for sharing data, what hope
might there be for sharing computer programs? While both data and software
together are required to validate outcomes of a computational analysis,
the do differ in the respect that the former can be seen as static/inert,
while the latter requires an environment and interaction. This makes 
software harder to share.
Our anectodal experiences
on this matter suggest that often a variety of reasons are given for why
code cannot be shared, e.g.~``there is no documentation / I do not
want to maintain it / I do not want to give away my code to my
competitors''. Our view, rooted in Open Science princples, is that
wherever possible sharing code is good for the community, as outlined
ten years ago \cite{Barnes2010-iv}. Having the code freely available, and
suitably archived, provides a valuable resource for others to learn
from, even when it doesn't run or if there is no documentation. However,
with a little effort, we believe that if someone independent can run/execute
the code, this is worth documenting as early as possible. With this in
mind, we have developed a set of PRINCIPLES and an example workflow that
provides what he hope is a pragmatic way of checking that code works.

The main contribution of this work is a thorough description of a process
and its variations to integrate a much needed evaluation of the 
computational reproducibility \cite{barba_terminologies_2018} into peer
review, and a demonstration of its feasibility by means of over 20
reproductions across different scientific disciplines.
We call this system CODECHECK as outlined next.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figs/rr.pdf}
  \caption{The inverse problem in reproducible research.  The left
  half of the diagram shows the diverse range of materials used
  internally within a laboratory.  These materials are often then
  condensed into a paper for sharing with the outside world via the
  research paper, a static PDF document.  Working backwards from the
  PDF to the underlying materials is impossible. This probihibits reuse
  and is not only non-transparent for a specific paper, but also 
  ineffective for science as a whole. By sharing the
  materials on the left, others outside the lab are able to reproduce
  or build on this work.}
  \label{fig:inverse}
\end{figure}

\section*{What is a CODECHECK?}\label{what-is-a-codecheck}

\subsection*{Workflow and people}\label{workflow-people}

CODECHECK is best demonstrated by way of our example workflow, and later
we expand on the underlying principles. The workflow involves three
groups of people: (1) the AUTHOR of a paper providing the code to be checked, (2)
the PUBLISHER of a journal intereseted in publishing the paper by the
AUTHOR and (3) the CODECHECKER who checks that the AUTHOR's code works.
The workflow that we have refined is documented in Figure~\ref{fig:workflow}.

% TODO: how is a CODECHECK different from a "specialist reviewer" ? Is it because it is not special, but regular?

\begin{figure}
% original figure source: https://docs.google.com/drawings/d/1tGOZFNZle-oE1Ynw_tLZFmNcv3wgxD0HPs5bXzDOrng/edit
% emojis: https://github.com/googlefonts/noto-emoji and https://github.com/twitter/twemoji
% emoji codes:
% book stack: 1f4da
% package: 1f4e6
% file drawer: 1f5c4
% disk: 1f4be
% document: 1f4c4
% phd: 1f393
% detective: u1f575
% laptop: 1f4bb
% folder: 1f4c1 1f4c2
% receipt: 1f9fe
  \centering
      \includegraphics[width=\textwidth]{figs/codecheck_overview.pdf}
  \caption{The CODECHECK example process implementation. Codechecker act as a detectives:
  The investigate and record, but do not fix issues.}
  \label{fig:worfklow}
\end{figure}

\textbf{Step 1:} the AUTHOR submit their manuscript along with code and data to
the PUBLISHER. The code and data need not be openly available at this point.
However, in many cases the code and data may be published on a code hosting platform,
such as GitHub or GitLab.

\textbf{Step 2:} the publisher finds a CODECHECKER to check the code. This is
analogous to the publisher finding one or more reviewers to evaluating
the paper. However, a cruical difference is that we suggest the CODECHECKER
and the author can talk freely to each other directly (see next).

\textbf{Step 3:} the CODECHECKER runs the code, based on instructions provided by
the author, and checks if some/all of the results from the paper can be
reproduced. (See later for exact nature of ``reproduced''). If there are
any problems running the code, the CODECHECKER asks the author for help
to resolve the problems. The CODECHECKER then tries to run the code
again. This process iterates until either the CODECHECKER is successful,
or the CODECHECKER concludes the code does not work (\emph{what do in this
case}). As part of this process, the CODECHECKER could work entirely
locally on their own compute resource, or in the cloud, e.g.~using the open
MyBinder infrastructure \cite{jupyter_binder_2018}. Using such cloud-based 
infrastructure allows for the CODECHECKER and author to collaboratively improve
the code and enforces a much more complete definition of the computing environment,
but, unless infrastructure is provided by, e.g., by the publisher, requires code
and data to be published openly online.

\textbf{Step 4:} the CODECHECKER writes a certificate stating how the code was
run and includes a copy of outputs (figures, tables) that were
independently generated. The certificate may include further recommendations to
the author how to improve the material.

\textbf{Step 5:} the certificate as well as auxiliary files created during the check,
e.g., a specifiction of a computing environment, helper scripts, and the code and data,
if available under open licenses, get deposited in an open archive (currently Zenodo);
the certificate is given to the PUBLISHER.

\textbf{Step 6:} the PUBLISHER can then use the certificate as part of their
supplementary records for a paper or provide it to scientific reviewers; the
PUBLISHER also gives credit to the CODECHECKER for their work by depositing the
activity appropriately in scholarly publication databases, such as ORCID.
The PULISHER also ensures proper publication metadata, e.g., links from the 
certificate repository to the published paper or the original code repository.

\subsection*{Variations}\label{variations}

\subsubsection*{Dimensions of CODECHECK workflows}\label{dimensions-of-workflows}

The example workflow leaves room for different
implementations of this process. To elaborate on these options, we consider
the following dimensions in a space of possible CODECHECK workflows, as shown in 
Figure~\ref{fig:dimensions}, to represent relevant aspects for the involved
stakeholders. These aspects touch on timing, responsibilities, and 
transparency and will be discussed in the following sections.
Each of these have their own pros and cons and can for the most part be
freely combined.

\begin{figure}
% original figure source: https://docs.google.com/drawings/d/1q2EdW3Ad-IpcD-CeoR1eIFMJW2fF6OVmDhCtCUmLO1k/edit
  \centering
      \includegraphics[width=0.8\textwidth]{figs/codecheck_dimensions.pdf}
  \caption{The dimensions of implementing a CODECHECK process.}
  \label{fig:dimensions}
\end{figure}

\subsubsection*{When to do a CODECHECK and with what power}
\label{when-to-do-a-codecheck}

The time for a CODECHECK greatly depends on the intentions and the ascribed
importance. The earlier a CODECHECK happens in the process, the more the
results can play a role in the most important decision in scholarly 
communication: is a paper published, sent back for revisions, or rejected.
The later in a review process the check happens, the less challenging is it
to allow a bidirectional communication between author and codechecker, e.g.,
because the author might already be notified of the acceptance and may more
freely share materials online.
Even earlier checks, i.e., a CODECHECK of a preprint, may even help to 
improve the workflow itself even before a publisher is involved. The 
possibility for codechecking papers could be part of a preprint server's
policy, or simply be initiated by interested readers and community members.
% TODO add a sentence about the possible effect of just asking for
% code/data early on in the submission process, see https://twitter.com/tsuyomiyakawa/status/1230672758163402752?s=09)

A pre-review CODECHECK allows editors to use it as a filter to only send 
a submission out to peer review if it passes the check. A less strict
pre-review check would simply be included in the submission package provided
to the reviewers, so they can include the CODECHECK's results in their 
evaluation of the work.
A CODECHECK may also be conducted in parallel to the scientific review. This
puts less burden on the turnaround time for the check yet only makes the 
outcomes available for the final consideration of the handling editor.
The check could also be tasked on demand, e.g., only if one of the reviewers
suggests/requests one because previous screenings might not have caught the 
fact that a computational workflow is part of the submission. However, 
soliciting such a "specialist review" is much more undesirable than having
a regular CODECHECK, without any special treatment for some submissions.
A CODECHECK could then be awared equal weight as the other reviews.
A post acceptance CODECHECK puts the least requirements on the author and may
simply lead to an award of excellence on top of the acceptance of the
submission. This is the least strict solution while still properly 
acknowleding the check results, because the CODECHECK is to be completed 
before the publication of the final paper.
The GIScience group of checks (see \nameref{register}) falls into this
category: the AGILE conference highlights articles where the reproducibility
was successfully reviewed with a badge on the volume and article landing
pages.

Finally, a CODECHECK may also be conducted after publication, though it
would required to update the article later with a note to reference the check,
otherwise readers would presumably not be aware of it. While this has the
least impact on the current publishing practices, it also has the smallest
effect and does not acknowledge the importance of reproducible workflows for
ensuring good scientific practice.

\subsubsection*{Who does the CODECHECK?}\label{who-does-the-codecheck}

The options for the person who can conduct a CODECHECK require a not quite
simple matching of a combination of skills and availability. Ideally, the
codechecker has a matching code \emph{and} domain expertise, for example to
check a workflow based on Python code analysing MRT brain imagery. However,
a well-documented workflow should be executable by any person with a basic
understanding of computers. Naturally, the more prerequisite knowledge the
codechecker has, the less time is spend to understand the goals and 
the mechanics of an analysis. From our experiences, the priority should be
given to matching technical expertise first, as the a lack of knowledge in
setting  up a computing environment with a particular language or tool is 
much more discouraging and the assessment of the outcome, e.g., comparing
created figures with the original, is largely possible without 
understanding.
The time and expertise alloted to the check
are the main drivers for the depth of checking, though in general, we 
expect a CODECHECK not to evaluate performance, e.g., does a racing car
have the best apex speed, but to assess operation free of failure, i.e., 
is it a car with 4 wheels and at least one door, and does it move forwards
when accelerating. This depth can be seen independent from the power of
the check (see above), as long as it is communicated clearly to all
stakeholders, most importantly authors and readers.

The concrete people to provide these expertises can range from researchers,
possibly drawn from the regular pool of volunteer reviewers or from a 
special group, to editorial staff of a publisher. If the regular reviewers
are relied upon, the editor's task to find suitable ones becomes more
tedious; extra information such as software familiarity would have to be
modelled in review platforms.
We see a great opportunity to especially involve early career 
researchers as codecheckers. They arguably have a high interest in learning
about the latest tools and technologies as they are building up their own
expertise and specialisation. They are also introduced to peer review and 
may transition into the role of scientific reviewer over time.
Not to solve the general problems of peer review, which is largely an
unsupervised process, i.e., rarely are reviewers taught how to do it, but
a junior codechecker may be supported by a senior codechecker, too.
We see a high potential in setting up the new processes for CODECHECKING with
a clear committment on opennes and transparency, independent of the
respective current peer review process (see \nameref{who-knows-who}).

Publisher staff represents the most controlled yet also not quickly scalable
option. Hiring the required technical and or domain expertise puts a financial
burden on the publisher, of course, but such a committment also shows the 
value given to publishing reproducibility and would positively impact the 
submission experience, because of more controllability, and external 
perception of the journal or conference. A staff codechecker reduces the 
challenges when it comes to open communication and awarding credit, as
in-house personnel may be more readily be given access to non-anonymised
submission material and receives a proper salary instead of public credit
as part of the scientific community.

In contrast, for researchers it can be very important to be publicly credited
with their activity as a reviewer (see \nameref{who-knows-who}). A regular
review may be listed in public databases (e.g., ORCID
\footnote{\href{https://support.orcid.org/hc/en-us/articles/360006971333-Peer-Review}{https://support.orcid.org/hc/en-us/articles/360006971333-Peer-Review}} or commercial offerings such as Publons
\footnote{\href{https://publons.com/}{https://publons.com/}} or 
ReviewerCredits\footnote{\href{https://www.reviewercredits.com/}{https://www.reviewercredits.com/}}).
A codechecker should be listed just the same. The number of scientific reviews
if of course a very coarse indicator for the community contribution by an 
individual researcher, so it's unclear if a regular reviewer, who in addition
conducts a CODECHECK to the scientific review, should be credited twice. 

The codecheckers community\footnote{\href{https://github.com/codecheckers/codecheckers/}{https://github.com/codecheckers/codecheckers/}}
currently has over 20 individuals who signed up as volunteer codecheckers
following talks given on the project in the last 12 months.
That is even though no checks are actively elicited.
We see a high potential in an open shared list of potential codecheckers 
compared to a private, in-house group, as it may better match expertise
and share the workload.
% TODO tell more about the motivations and commitments that we know about?
The overarching community is also a mean to ensure CODECHECK is a viable
option not just for large publishers, independent of their financial model,
but also for independent no-cost open access journals.

\subsubsection*{Who knows who?}\label{who-knows-who}

The topic of blinding is broadly discussed, not the least in the push towards
open peer review as part of the Open Science movement [REF]. Without taking
a strong stand on this discussion, the general motivation for more 
transparency and reproducibility does indeed favour a more open review 
process. However, anonymity can also serve to protect individuals 
\ref{tennant_limitations_2020}, e.g.,  juniour scientists, whereas it might 
be acceptable to require revealing of the names of tenured researchers.
The possible negative effects of a non-anonymous review are greatly reduced
if a CODECHECK is not relevant for the decision to accept or reject, but that
is of course not desirable. Instead, we argue that a CODECHECK is technical
process that should in any case be fixable and not a question of opinion or
faulty approach. It is possible for a nonsense workflow to receive a 
CODECHECK certificate.
Furthermore, the level of documentation that is needed for third parties
to reproduce a workflow is extremely hard to get right. Too often, this 
uncertainty leads to researchers giving up and not documenting at all.

The technical nature of the check and the challenge of good enough 
documentation is why we see great benefits in a effective bidirectional
means of communication between author and codechecker. Instead of trying to
fix problems or guess the next step, the codechecker can ask the author to 
rework the docs  or the code.
Instead of lenghtily describing a small change, the codechecker may simply
provide a fix within the code.
Instead of hiding useful and  instructive files created during the codecheck
(e.g., a machine-readable computing environment specification), the author 
and readers can profit from the software-related expertise of the
codechecker.
While such a communication may be facilitated in an anonymous way by the 
publisher, it most likely only helps to protect the identity of the 
codechecker, because, especially when of a certain size, not trivial, or 
if very specialised, it is close to impossible to fully anonymise a 
software analysis stack.
Therefore, the most effective means with the most desirable result for
the stakeholders is a non-anonymous and collaborative CODECHECK.
The possible contributions by the codechecker may even be integrated into
the code of the workflow and be acknowledged as code commits. This way, 
proper credit is given within the research software development community.
Nevertheless, depending on other needs represented by other dimensions,
the aspect of `Who knows who?' may be the most flexible one.

\subsubsection*{How is the CODECHECK integrated in other processes?}\label{turnaround-time}

As a prerequisite, we see a CODECHECK as part of a typical scholarly
communication process. While more radical ideas to change the paradigms
how researchers share their work exist\footnote{For example Octopus, 
\url{https://science-octopus.org/about}, or Hypergraph
\url{https://www.libscie.org/hypergraph}.}, there is a 
value in an evolutionary approach. Thereby, existing communities can 
transition towards more open practices at their own pace and ensure
no one is left behind unfairly.
When integrating a CODECHECK in existing review and publication processes,
the \emph{turnaround time} is crucial. Depending on when and who conducts
the check, it may be done faster, or add considerably to the overall
time from submission to publication.
% TODO write about our experiences how long a CODECHECK takes?

\section*{Core principles}\label{core-principles}

The example workflow and variations just outlined above reflect our current
views on how a codecheck should be performed. They are not set in
stone, but we do believe the following core principles underpin our
CODECHECK:

\textbf{1. Codecheckers record but don't investigate or fix.} The
codechecker follows the instructions provided by the author in running
the code. If any steps are unclear, or if code does not run correctly,
the codechecker records this fact, and then communicates this
information to the author. We believe that the job of the codechecker is
not to fix all these problems, but simply to report them, and await a
fix from the author.

\textbf{2. Communication between humans is key.} Some code may just work
without any interaction (e.g.~Manuel's certificate TODO add cert number); 
however, more often
there are hidden dependencies that need to be found, or revised software
installations. Allowing the codechecker and human to communicate
directly and openly with each other should make this process as
constructive as possible. We feel that routing this conversation
(possibly anonymously) through a publisher would slow things down and
reduce the chances for community building as well as the lessons learned
for both author and codecheckers.

\textbf{3. Credit is given to codecheckers.} The value of contributing a
CODECHECK is comparable to providing a scientific peer review. Indiviual
variations aside, it may also require a similar amount of time. Therefore,
the codechecker's activity should be publicly recorded and mentioned in the
published paper.
The public record can be realised by publishing the certificate in a 
citable form (i.e., with a DOI; this is done within the community codecheck
process), by listing codecheckers on the journal's website or, ideally, by
publishing the checks alongside peer review activities in public databases.

\textbf{4. Workflows must be auditable.} This requires that the codechecker 
has at least enough material to validate the workflow outputs submitted by 
the authors. Stark~\cite{stark_before_2018} calls this `preproducibility'.
Code could be executed at least once without critical errors or
warnings using the provided instructions, and, therefore, all code and data 
was given and could be investigated more deeply or extended in the future.
Ideally, this is a "one~click" step, but achieving this requires particular 
skills. It is also hard to create a sufficient level of documentation for 
third parties. That is why the possibility to communicate is more important
than perfected workflows, and why a CODECHECK is not automated.
A codechecker does \emph{not} conduct a code review.
% more content from the website, may fit better elsewhere:
% The CODECHECK is not automated on purpose: automation may (a) lead to people gaming the system, (b) hide details that eventually decrease level of certainty that a codechecker has in their assessment, and (c) reduce the understandability of instructions in the long term, which is more important than short term ease of use (see Principle 2).

\textbf{5. Open by default.}  By default, unless there are strong
reasons to the contrary (e.g., clinically-sensitive data), all code and
data, both from author and codechecker, will be made freely available, i.e., 
under open licenses, before or when the certificate is published.
The code and data  publication should follow community good practices.

\section*{Implementation}\label{register}

\subsection*{Register}\label{register}
% count certificates: 
% curl https://codecheck.org.uk/register/stats.json
% -1 for Hathway-Goodman (no empty last line, so no -1 for the column headers needed)

We have a curated list of 22 certificates available at
\url{https://codecheck.org.uk/register}. These are a mixture of
reproductions of papers for a variety of reasons and fall into three themes:
(1) `classic' papers from computational neuroscience
(2) COVID-19 modelling preprints, and
(3) GIScience.  
The first theme was the initial set of papers used to explore the concept
of CODECHECK. The idea was to take very well known historical articles from a 
specific domain, in this case the work area of co-author S.~Eglen, possibly 
as as a conversation starter for potential collaborating journals.
The second theme is rather opportunistic. As the COVID-19 pandemic swept
over the whole Earth, we hoped to contribute a small part to science's answer
to the unknown virus. The checks were solicited through community interaction
%Several COVID-19 models have been done during the time of peer review of these papers,
and under our initative rather than requested from journal, but the certificate
¸has then been acknowledged/cited in the paper (LANCET/LSHTM). % TODO SJE: clarify references in brackets
The third theme represents co-author D.~N\"ust's service as Reproducibility
Reviewer at the AGILE conference series.
Independent of CODECHECK, the \emph{Reproducible AGILE} Initiative \cite{reproducible_agile}
established a process for reproducing workflows at the AGILE conference
series \cite{nust_improving_2020}.
While using slightly different terms and infrastructue, e.g., ``reproducibility
reports'' are published on OSF instead of certificates on Zenodo, the 
reproducibility reviews adhere to the codecheck principles.
Furthermore, a small number of checks was completed as part of a peer reviews
for a GIScience journals.

Table~\ref{tab:register} lists the certificates completed to date,
along with citations to the original papers.
Out of these areas, we would like to briefly highlight three examples here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Gigascience -- our first paper. Only visualisation, not machine
  learning (which would have taken sseveral days of compute time).
\item
  report 9 -- covid model of UK lockdown model -- unlike many claims
  in the popular media at the time, the model was reproducible
  [NOORDEN news piece]. 
\item
  Manuel's certificate -- approached us as he requested a certificate
  as paper was entering peer review.
\end{enumerate}

\begin{table}
  \centering

  \begin{tabular}{llp{11cm}}
    \toprule
    Certificate & Research area & Description \\ \midrule
    2020-001 \cite{cert-2020-001} &
    Machine learning & CODECHECK undertaken post acceptance of
    manuscript and before its publication in \textit{Gigascience}
    \cite{Piccolo2020-lo}. \\
    2020-002  \cite{cert-2020-002} & neuroscience & Paper from 1997
 showing unsupervised learning from natural images \cite{Hancock1992-mp}.
 Code written for this project. \\
    2020-003  \cite{cert-2020-003} & neuroscience &  Classic paper
                                                     from 1982 on
                                                     models of
                                                     associative
                                                     memory
                                                     \cite{Hopfield1982-mz}.
    (Simulation code was written specifically for this project.)\\
    2020-004  \cite{cert-2020-004} & area & Cart-pole balancing problem solved using reinforcement learning \cite{Barto1983-rg}.  Code written for this reimplementation.\\
    2020-005  \cite{cert-2020-005} & area & Larisch \\
    2020-006  \cite{cert-2020-006} & area & Detorakis \\
    2020-007  \cite{cert-2020-007} & area & in progress \\
    2020-008  \cite{cert-2020-008} & COVID-19 & Modelling of
    interventions on COVID-19 cases in the UK; checked as preprint (NOW REMOVED XXX)
    and now published \cite{Davies2020-vj} \\
    2020-009  \cite{cert-2020-009} & COVID-19 & tracing \\
    2020-010  \cite{cert-2020-010} & COVID-19 & report 9 \\
    2020-011  \cite{cert-2020-011} & COVID-19 & Modelling of Covid
    spread across Europe;  paper was checked when in press (now  published) \cite{Flaxman2020-yb}. \\
    2020-012  \cite{cert-2020-012} & COVID-19 & Modelling of COVID-19 spread across the USA; preprint currently review \cite{Unwin2020}. \\
    2020-013  \cite{cert-2020-013} & area & biorXiv \cite{Spitschan2020.06.02.129502}. \\
    2020-014  \cite{cert-2020-014} & area & eLife \cite{Sadeh2020}. \\
    2020-015  \cite{cert-2020-015} & area & eLife \cite{Liou2020}. \\
    2020-016  \cite{cert-2020-016} & GIScience & 
    R models demonstrating the Modifiable Aral Unit Problem (MAUP) in spatial data science \cite{Brunsdon2020}. \\
    2020-017  \cite{cert-2020-017} & GIScience & 
    Spatial data handling, analysis, and visualisation using a variety of R packages \cite{Bivand2020}. \\
    2020-018  \cite{cert-2020-018} & GIScience & 
    Cellular automaton in R for modeling dynamic phenomena within a Discrete Global Grid System (DGGS) data model; using a subset for demonstration \cite{Hojati2020}. \\
    2020-019  \cite{cert-2020-019} & GIScience & Reachability analysis of suburban transportation using analysis of privately owned shared cars; workflow uses Python, and reproduction with a considerably subsampled dataset \cite{Illium2020}. \\
    2020-020  \cite{cert-2020-020} & GIScience & In-database windows operators for processing spatio-temporal data streams based on Python and PostgreSQL/PostGIS \cite{Werner2020}; reproduction using container. \\
    2020-021  \cite{cert-2020-021} & GIScience & Python code for comparing supervised machine learning models for spatial nominal entity recognition. \cite{Medad2020}. \\
    2020-022  \cite{cert-2020-022} & GIScience & Python code for visualising text analysis on intents and concepts from geo-analytic questions \cite{Xu2020}. \\
    2020-023  \cite{cert-2020-023} & GIScience & Analysis of spatial footprints of geo-tagged extreme weather events from social media using R \cite{Owuor2020}. \\
    \\ \bottomrule
  \end{tabular}
  \caption{Register of completed certificates as of October 2020.  An interactive version
  is available at \url{http://codecheck.org.uk/register}. % TODO: SJE to complete...
  }
  \label{tab:register}
\end{table}

\subsection*{Annotated certificate}\label{annotated-certificate}

At the completion of each CODECHECK, a certificate is written that
states which outputs, i.e., numbers, figures and tables, from the 
original article could be reproduced by the codechecker.
This certificate is made openly
available so that all readers of the article can see which elements
have been reproduced.  The certificate also includes links to the code
and data used by the codechecker, allowing others to build on the
work.
The format of the certificates has evolved slightly during the
project, as we have learnt to automate different aspects of the
certification.  

To demonstrate the typical contents of a certificate, in
Figure~\ref{fig:cert} we have annotated the first four out of ten
pages of the certificate \texttt{2020-012} \cite{cert-2020-012}.
The referenced article \cite{Unwin2020} describes the modelling COVID-19
across the USA.
%% TODO - has this been published yet?.  No, not yet... in review...
Figure~\ref{fig:cert}A shows the certificate number, and its DOI,
which points to the certificate as archived on Zenodo.
The CODECHECK logo is
attached to the certificate to denote successful reproduction.
Figure~\ref{fig:cert}B provides the key metadata about the check --
which paper was checked, who the authors and the codechecker were,
and when it was checked. Materials
relating to the codecheck are freely available via the Repository
link. This table is generated from machine readable text file, the
CODECHECK configuration file \texttt{codecheck.yml}.
The configuration file collects the core pieces of metadata surrounding
a check and enables current and future automation of workflows and 
meta-analyses.
Figure~\ref{fig:cert}C is a textual summary of how the codecheck was
performed, and any interesting findings.

Figure~\ref{fig:cert}D (page 2 of the certificate) shows a table that
lists the outputs that were generated from the codecheck (this is
termed the MANIFEST).  For each output, there is a comment stating
which figure/table it should be compared to in the original paper,
along with the size of the file.  On page 3 of the certificate,
Figure~\ref{fig:cert}E shows some more detailed notes from the
codechecker, in this case indicating what steps were needed to
initialize the environment and run the code.  This particular example
was quite long, taking about 17 hours to run.  Finally, on page 4 of
the certificate, we see the first output that was generated by the
codecheck (Figure~\ref{fig:cert}F).  In this case, the figure matches
figure~4 of \cite{Unwin2020}.  Subsequent pages of the certificate show other
outputs (not shown here).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/annotate-cert-crop.pdf}
  \caption{Annotated certificate 2020-012 \cite{cert-2020-012} (first four pages only).}
  \label{fig:cert}
\end{figure}

\subsection*{Tools and resources}\label{tools}

To run the system, we rely on freely available infrastructure, more 
specifically GitHub and Zenodo.

The \texttt{codecheckers} GitHub organisation\footnote{
\url{https://github.com/codecheckers}} contains projects managing the
project website, the community of codecheckers, community discussions,
forks of codechecked code repositories, and the main register for
managing the collection of checks. Both the website\footnote{
\url{https://codecheck.org.uk/}} and the register
\footnote{\url{https://codecheck.org.uk/register}} are hosted as GitHub
pages. The register is effectively a single table in CSV format that
connects the certificate identifier with the repository associated with
a specific conducted codecheck. Each of these repositories, which 
currently can be hosted on GitHub or Open Science Framework (OSF), 
contains the CODECHECK metadata file \texttt{codecheck.yml}. The register
further contains a column for the type of check, e.g., community, journal,
or conference, and the respective GitHub issue where communications and 
assignments around a specific check are organised. No information is 
duplicated between the register and the metadata files. When a new
certificate is entered into the register, an automated process using 
continuous integration infrastructure of GitHub, GitHub Actions, retrieves
the metadata files and builds different representations of the register
for viewing (HTML) or integration with other services (CSV, JSON).

Zenodo is an open repository for scientific data and documents. It
mints DOIs for deposits and ensures long-term availability of all digital
artefacts related to the project. These include the community 
certificates as well as the register, which is regularly archived to 
Zenodo \cite{codecheck_register} (including all file formats).

A custom R package, \texttt{codecheck}\footnote{
\url{https://github.com/codecheckers/codecheck}}, helps with the authoring
of certificates, and with the deposition of certificates and reated files to
Zenodo using the R package \texttt{zen4R} \cite{zen4r}.
Codecheckers may choose to not use the package and rely on their own tools
for creating the certificate. This is a deliberate flexibility to
accomodate both different skill sets and unforseen technical advances or
challenges.

These tools and resources demonstrate that a CODECHECK process can be
managed without infrastructure costs on freely available platforms.
The scripted and automated parts should also ensure that more checks can
be conducted in the future.
The remaining costs are the project's domain, which is negligible, and
the human resources for managing the project and processes, which are
considerable and currently soley based on, partly grant-based, public funding.
Naturally, similar set-ups are possible, such as using GitLab/GitLab.com 
and the GitLab CI instead of GitHub and GitHub Actions, or using a different 
data repository for depositing certificates and data.

\section*{Related work}\label{related-work}

TO BE CONTINUED.

All the related projectsfrom A-Z\ldots{} e.g.~CASCAID, Rescience,
modeldb, o2r. Community iniatives like reprohack along similar lines of
checking code. \url{https://reprohack.github.io/reprohack-hq/}

\url{https://twitter.com/Code_Copilot}

% first glimpse into "Code exeuction processes in scholarly publishing" ?
% `https://docs.google.com/document/d/1fMWrFvBTwAYXg5J_bYIjgb-gY7DfQ0DOjx9Vlg_Rbmg/edit# ?

DELIBERATEY NOT COMPREHENSIVE HERE.

\section*{Limitations}\label{limitations}

\textbf{Just peer review}: wait, isn't CODECHECK what peer review should be 
doing already? That is correct.
However, the general observation is that the increasing
number of researchers, submissions, and reviews to conduct under the
pressure to publish, puts the scholarly publication system under a lot
of strain [REF].  We also see parallels to the terms 
`reproducible research' and `open science'. Establishing a CODECHECK 
process is largely an acknowledgement that peer reviewing practices 
have faltered to adapt to the challenges of digitisation and computer-based
research or data science. The concept of a CODECHECK, just as the 
reproducibility of research and the openness of science, may be of 
transitional nature. If the activites described here as being part of a 
CODECHECK become the norm for scientific reviews and publications, then
the initiative will have succeeded.
However, the rapidly changing technology used in research leads us to 
believe that a seperate role of codechecker and the expertise it brings
will be benefitial in the long term.

\textbf{Not going deep enough}: 
It should be noted that a codechecker is not taking the same roles as the
\emph{statistical reviewer}, as it is practices by some journals in the 
biomedical domain (cf.~\cite{petrovecki_role_2009,greenwood_how_2015}).
The statistical reviewer actually evaluates the appropriateness of
statistical methods \cite{greenwood_how_2015} and can support topical
reviewers if, e.g., complex methods or sophisticated variants of statistical
tests are applied \cite{petrovecki_role_2009}.
The codechecker may go equally deep into the review, if they have, by 
intention of by chance, the required expertise and the required time, 
but they are by no means expected to be able to provide
such a level of scrutiny. We can imagine a tiered workflow where the 
codechecker, just as the conventional reviewer, could recommend a detailed
statistical review to the editor as they might come across upon different
hints.
%The CODECHECK provides a certain level of safety with regard to the inherent
%problems that a computational workflow might have. We want to capture the
%obvious errors, but ... https://www.quora.com/What-is-the-difference-between-safety-and-security

\textbf{Handling failure}: we have no established process for the case
that a reproduction fails. This may be the sign of a pre-selection bias
of the examples, as so far all our codechecks succeeded.
In case of a journal adopting CODECHECK for all submissions, the open 
question remains as what to do in this case will have to be answered.
At the start, we doubt publicly reporting failures (i.e., the code 
would not run) will increase overall reproducibility and recommend not
to share the outcome publicly but to share the latest draft of the 
certific with the authors.
One must be careful as to the impact of acceptance by authors and the
concerns of volunteers this approach, or a contrary one, has.

\textbf{Compute time}: for those papers that take significant compute
time (think days, not minutes), who will pay for the compute time?
The openness of CODECHECK certificates could help to establish community
peer pressure, as submitting authors can be evaluated on their engagement
as codecheckers.

\textbf{Proprietary software}: authors can currently provide code that requires
proprietary software. Given the prevalence of proprietary software in
some disciplines, such as MATLAB or eCognition,
this seems like a pragmatic choice.
However, it prohibits to benefit from open infrastructure for reproducibility
(cf.~\cite{konkol_publishing_2020}) and requires the codechecker to have access 
to that software.
Non-open software also considerably hampers reuse, especially by researchers
from the global south. Therefore, allowing proprietary software is a compromise
that each implementation of CODECHECK should critically reconsider.

\textbf{Can't someone cheat?} Yes. We don't check it is correct code,
just simply that it runs. This `mechanical' test is indeed a very low bar.
Yet, by having the code and (raw) data openly deposited, interested parties can
then examine the code, and knowing that the code will be open we hope will
be an incentive for authors to share. It will also give the opportunity 
to future researchers, potentially with new methods, to look for errors.
This is a much more effective means to avoid cheating than an arms race
trying to catch the cheater with closed datasets and code. This approach
is comparable to the storing of blood samples in sports to detect
doping in the future (cf. \cite{everythinghertz97}).
Another picture that helped us to define the scope of a CODECHECK is:
the codechecker is the forensic photographer, making sure all the details
are captured so that an investigator may, now or in the future, scrutinize
them.

\textbf{Who's got time for more peer review?} Agree, but the technical
nature and specific skill set might allow to introduce different groups
into the peer review process. ECRs might be attracted to working with
groups to learn more about recent methods and reproducibility practices.
Research software experts (Research Software Engineers, RSE) who might
not regularly be involved in writing or reviewing papers could be 
interested to increase their connection with scholarly practices.
An extra codechecker may simplify the matchmaking an editor has to do
when identifying suitable reviewers for a submission, as technical and
topical expertise can be provided by different people, who must not even
be traditional peer reviewers. In fact, the codechecker could even be
a software professional employed by a publisher.
Also, recall that we expect CODECHECKSs to be not anonymous and always
publicly deposited, which is common practice for some journals, but not
broadly practiced. We also found that the focus on the mechanics of a
workflow and the ability to interact with the authors can make
reproductions very educational. It's a different role and as such could
be a welcome alternation/diversion for reviewers.

\textbf{Who finds the codechecker?} Good point! The journal, using either
their own team of codecheckers or the public database. Community
however think this is an important issue, and are volunteering.

\textbf{Checking multiple times?} If you check a preprint before peer
review or a submission to a journal with non-public review,
it might need checking again as it is modified during peer
review.  This is in part unavoidable, and as happened to us (e.g., 
`2020-012` has been revised). This can also be desired, if the
interaction between author, reviewer, and codechecker lead to
improvements.
Since checking the manuscript the second time is likely to be
much less work than the first time, this is largely mitigated by 
open and regular communication.

As a principle, we do not require results to be exactly the same for a
codecheck to pass, simply that the code runs and generates the output
files that the author claims. Stochastic simulations mean that often
we cannot get exactly the same, or even different versions of
libraries.  (Cite freesurfer example \cite{Gronenschild2012-pp} that
generates different outputs on different operating systems).

\begin{itemize}
\item modelDB principle: minimum of one figure should be reproducibe.
  Example certificate 2020-016 (I think) which had only a small subset
  of the possible figures. Still useful and worth sharing.
\end{itemize}

\section*{Future work and conclusions}\label{future-work-and-conclusions}

CODECHECK works!
We have created a considerable number of certificates to demonstrate it.
The creation of the certificates and the interactions with authors and
editors not only improved the principles and the workflow, but also
confirmed the approach taken. CODECHECKs and can increase trust in
scientific results. Yet, they underly the same limitations as peer review
in general and are closely connected to larger disruptions and challenges
in scholarly communication \cite{eglen_recent_2018,tennant_ten_2019}, 
including the tensions between commercial publishing and reviewer's often
free labour.
Even more, it is clear that establishing CODECHECK-like systems can be 
impeded by and must be seen in connection to much larger issues in 
science, such as broken metrics or malpractices triggered by publication
pressure \cite{piwowar_altmetrics:_2013,nosek_promoting_2015}.
While developed for the current "paper"-centric publication process and
being compatible with different styles of peer review, the CODECHECK 
principles would also work well with novel publication paradigms that enable
much more iterative and granular communication of scientific outputs,
e.g., Octopus \cite{}. An explicit segmentation of research steps could
even make the focus of a CODECHECK, e.g., on the analysis sub-publication,
easier.

If adopted today, a CODECHECK system, even if temporal in a sustainable 
transition towards more open publication and review practices, can contribute
to increased trust in the results of research.
More reproducible practices initiated by CODECHECKs could lead
communities/disciplines to reach a state where authors do provide good
enough material and reviewers have acquired broady enough skills that
the latter will generally conduct a CODECHECK-level of checking, and
only in especially sophisticated cases will a specialised codechecker
be included.
The main challenge for us remains to get journals to embrace the
idea behind CODECHECK, whether they use the name for it or not.
This \emph{cultural change}, however, is needed for the valuation of the
efforts that go into proper evaluation of papers.
With the implementation of CODECHECK workflows, the question of training
codecheckers will become relevant. We expect both a mentoring scheme within
the CODECHECK community (experienced codecheckers provide on the job training
or serve as fallback advisors) as well as collaboration with reproducible
research initiatives such as
ReproHack\footnote{https://reprohack.github.io/reprohack-hq/},
ReproducibiliTea\footnote{https://reproducibilitea.org/}
\cite{fitzgibbon_brewing_2020},
and Repro4Everyone\footnote{https://repro4everyone.org/}
\cite{auer_reproducibility_2020},
to be able to grow the pool of codecheckers as needed.
The initial reaction of the scientific community, where up to today over 
twenty people spontaneously signed up as volunteer codecheckers after talks
on the initiative, makes us feel optimistic that the time is right for 
putting scholarly peer review on the path to facilitate sharing and execution
of computer programs and documenting reproducibility of results.


\begin{itemize}
\item
  
\item 
  
\item
  possibility to allow CI for open papers such that for ``nearly free''
  we can find out when softwave breaks, and if provenance tools are
  used, when.
\item
  How to embed this in journals.
\item
  When some journals have it - what are crucial decisions/pain points?
  How open do they make the check itself and its outputs, and how does it
  relate/influence the openness of regular peer review
\item
  When is CODECHECK redundant? will it become part of regular peer
  review? or, if ayuthors become more skillful, then everything becomes
  automated, and no need for codecheck. CODECHECK disappearing is then a
  measure of success.
\item
  codecheck allows innovation independent of finance model of the journal
  and current peer review practices; evolutionary now, the "norm" later
\end{itemize}

Or will it always be adjunct to peer review, and needed?

working within the trad system of publishers to help push the
conversation along.




\subsection*{Competing interests}

SJE is on the senior editorial board at the journal \emph{Scientific
Data}.
DN is reproducibility editor at the Association of Geographic Information 
Laboratories in Europe's (AGILE) annual conference.

\subsection*{Grant information}

This work was financially supported by the UK Software
Sustainability Institute and a Mozilla Science mini grant.
DN is supported by grant
\href{https://gepris.dfg.de/gepris/projekt/415851837}{PE~1632/17-1}
from the German Research Foundation (DFG).

\subsection*{Acknowledgements}\label{acknowledgements}
%Collaborators and potential collaborators:

We are grateful to the following individuals for discussions regarding
the work presented here: Andy Collings, Melissa Harrison, Giuliano
Maciocci (eLife), Rebecca Kirk (PLOS Computational Biology), Scott
Edmunds (GigaScience), and Andrew Hufton (Scientific Data). Iain Davies and
Yuhao (Sebastian) Wang developed code and example certificates.

\subsection*{Author contributions}

DN and SJE contributed equally to all aspects of this project.

{\small\bibliographystyle{unsrtnat}
\bibliography{bibliography}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\begin{itemize}
\item
  The Lancet's statistical review process: areas for improvement by authors
  \url{https://www.thelancet.com/journals/lancet/article/PII0140-6736(92)90409-V/fulltext}
\item
  data enclaves
\item
  CASCAID
\item
  ROpenSci/PyOpenSci (and how their software review differs, though they
  also collaborate with journals,
  \url{https://devguide.ropensci.org/softwarereviewintro.html})
\item
  time capsules \textgreater{}
  \url{https://twitter.com/DougBlank/status/1135904909663068165?s=09}
\item
  science is changing:
  \url{https://theconversation.com/what-you-need-to-know-about-how-coronavirus-is-changing-science-137641}
\item
  reproducibility is important, also

  \begin{itemize}
  \item
    effects on well-being/mental health,
    \url{https://www.nature.com/articles/d41586-020-01642-9}
  \end{itemize}
\end{itemize}

\textbf{main contributions}

\begin{itemize}
\item
  a concept for integrating a minimal code review into common scholarly
  communication processes around peer review
\item
  set of shared principles (recognition value) that allow researchers to
  quickly understand the level of evaluation across
  journals/publishers/conferences, and allows helps these stakeholders
  to establish workflow checking in their domain/solution/product
\item
  a common language and rubric for classifying different levels of code
  and data evaluation as part of a peer-review
\end{itemize}

\section*{What is CODE CHECK?}\label{what-is-code-check}

\begin{itemize}
\item
  \textbf{Principles}: \url{https://codecheck.org.uk/}
\item
  relation to ``proper'' citation of data and software, and depositing
  of data and software in suitable repos \emph{besides} the bundle for
  CODE CHECK: do it, only provide the concrete analysis script for the
  check
\item
  \textbf{Why is it useful?}

  \begin{itemize}
  \item
    reduce the barrier to evaluating non-text parts of research
  \item
    acknowledges different skill sets and existing high load on
    reviewers
  \item
    CC breaks problem that publishers staff expertise today is not
    likely to suffice
  \item
    engaging ECRs is a perfet fit (they are up-to-date in methods, are
    introduced to peer review)
  \item
    relation to research compendia? point it out for authors unsure
    about how to structure their workflow
  \item
    shifting burden from reviewer to author (but not too much)
  \item
    What issues with (open) peer review does CODECHECK address?
  \end{itemize}
\item
  keynote \url{https://doi.org/10.7557/5.4963}: in discussion, the
  speaker mentioned value of peer review for ``baseline reassurance'',
  i.e.~a paper has at least been checked by someone with an
  understanding, to increase trust especially if looking at papers from
  other disciplines
\item
  \textbf{report} must fulfil some requirements to make clear what is
  really checked and not mislead - readers will have very different
  understandings of a green checkmark!
\item
  reproducibility is hard
  \url{https://twitter.com/annakrystalli/status/1144176149859377152?ref_src=twsrc\%5Etfw}
  \textgreater{} code check gives at least some benefits
\item
  what are the requirements / level or reproducibility required or
  checked?

  \begin{itemize}
  \item
    \href{https://stodden.net/icerm_report.pdf}{Stodden et al., 2013}:
    five-level hierarchy of research - reviewable, replicable,
    confirmable, auditable, and open or reproducible
  \item
    \href{https://figshare.com/articles/Showing_your_working_A_guide_to_reproducible_neuroimaging_analyses/4244996/2}{Whitaker
    2017}: reproducible, generalisable, replicability, robustnes
  \end{itemize}
\item
  can authors retract their code/data once a CODECHECK has started? In
  the eLife prepreint review, there seems no ``way back'', see
  \url{https://twitter.com/ceptional/status/1271528976763121664?s=09}
\end{itemize}

\section*{Related work}\label{related-work-1}

\begin{itemize}
\item The limitations to our understanding of peer review: https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-020-00092-1
\item
  CODECHECK vs.~Verification Reports \textgreater{} \textgreater{}
  \url{https://www.sciencedirect.com/science/article/pii/S0010945220301738\#fn3}
\item
  CODECHECK vs.~the ``Artifacts Evaluated - Functional'' badge of the
  ACM, see
  \url{https://www.acm.org/publications/policies/artifact-review-badging}
\item
  Does CODECHECK do internal replication, as defined by
  \url{https://gatesopenresearch.org/articles/4-17/v2} ?
\item
  \url{https://paperpile.com/shared/rVNwBS}
\item
  Are ``Replicated Computations Results (RCR) Report'' a thing?
  apparently so (using ACM terminology)

  \begin{itemize}
  \item
    \url{https://dl.acm.org/doi/10.1145/3185337}
  \item
    \url{http://users.iems.northwestern.edu/~nelsonb/Publications/Nelson_RCRgreenSim.pdf}
  \item
    \url{https://dl.acm.org/doi/epdf/10.1145/3341094}

    \begin{itemize}
    \item
      citation for replicated workflow lists it as ``to appear''
    \end{itemize}
  \item
    getting proper papers for reproductions != CODECHECK approach where
    we want credit as reviewers
  \item
    see also
    \href{https://www.google.com/search?q=\%22Replicated+Computations+Results+\%28RCR\%29+Report+for\%22}{Google
    Search}
  \end{itemize}
\item
  \url{https://medium.com/bits-and-behavior/a-modern-vision-for-peer-review-d5f73f0fae07}
  - work by Amy J. Ko:
  \url{https://faculty.washington.edu/ajko/publications}

  \begin{itemize}
  \item
    is CODE CHECK's process too small, too incremental a change?
  \end{itemize}
\item
  overview of peer review and bibliometrics (if not a reference itself,
  it gives plenty of references):
  \url{http://juser.fz-juelich.de/record/865096/files/Peer\%20Review\%20and\%20Bibliometrics.pdf}
\item
  Open Peer Review \textgreater{}
  \url{https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-019-0063-9}

  \begin{itemize}
  \item
    Are OPR lessons transferable to CODECHECK? Does CODECHECK work
    better with OPR than with (single/double) blind peer review?
  \item
    similarities to their advice: open reports, give credit
  \end{itemize}
\item
  \emph{Successes and struggles with computational reproducibility:
  Lessons from the Fragile Families Challenge} \textgreater{}
  \url{https://osf.io/preprints/socarxiv/g3pdb/}
\item
  community standards for software, cf.~slide 6 in
  \url{https://figshare.com/articles/_/5675104}

  \begin{itemize}
  \item
    ``good enough practice'' is ``get a colleague to try using it'',
    slide 19 in \url{https://figshare.com/articles/_/5675104}
  \end{itemize}
\item
  journals doing code reviews

  \begin{itemize}
  \item
    \emph{``We know of 16 journals that take this valuable step (ie TOP
    Level 3): AEA as already mentioned, plus others in polisci,
    psychology, biostats, and org chem: see Data Rep Policies here
    \url{https://t.co/plOF8j6ADU}''}
    (\url{https://twitter.com/EvoMellor/status/1202692360456589339?s=09});
    see also whole thread!
  \item
    \url{https://medium.com/@NeurIPSConf/call-for-papers-689294418f43}
  \item
    \url{https://www.journals.elsevier.com/information-systems/editorial-board/}
  \item
    SIGMOD Reproducibility Review?
  \item
    Biostatistics ``AER''
    (\url{https://academic.oup.com/biostatistics/article/10/3/405/293660})
  \end{itemize}
\item
  open science platforms/cloud services \textgreater{}
  \url{https://www.nature.com/articles/d41586-019-03366-x}
\item
  live code in articles \textgreater{} o2r, eLife RDS
  \url{https://www.nature.com/articles/d41586-019-00724-7}, \ldots{}
\item
  Artefact Evaluation - \url{https://www.artifact-eval.org/about.html}
\item
  continuous analysis - \url{https://www.nature.com/articles/nbt.3780}
\item
  ``A Universal Identifier for Computational Results'' \textgreater{}
  \url{https://www.sciencedirect.com/science/article/pii/S1877050911001256}
\item
  Discussion about using Docker containers during JOSS reviews
  \textgreater{}
  \url{https://github.com/openjournals/joss/issues/498\#issuecomment-462046912}
\item
  \url{https://scigen.report}, an independent site for registering
  reproductions, via
  \url{https://annakrystalli.me/talks/ro-reprohack.html\#32}

  \begin{itemize}
  \item
    example
    \url{https://scigen.report/reviews/get?doi=10.1063/1.1823034}
  \item
    seems to be closed infrastructure
  \end{itemize}
\item
  new journal \emph{ReScienceX} (about experiments) also seems to have
  an ECR angle, \url{https://www.nature.com/articles/d41586-020-01328-2}
\item
  really interesting work on reproductions in hydrology, with a neat
  survey that reproducers filled out which might be interesting to
  capture from codecheckers, too:
  \url{https://www.nature.com/articles/sdata201930\#Sec9}
\end{itemize}

\section*{CODE CHECK implementation concepts}\label{code-check-implementation-concepts}

\begin{itemize}
\item
  need to get wording straight: process/implementation/workflow (DN:
  suggest to use ``workflow'' for research-analysis workflow)
\item
  \url{https://codecheck.org.uk/process/}
\item
  \emph{dimensions} along which a check can vary
\item
  independent/pre-submission (Peer\_Coding) by author + colleagues
\item
  AGILE
\item
  premier (OA) publisher
\item
  community OA / scholarly society with public peer review
\item
  invited reproducibility paper track,
  cf.~\url{https://www.elsevier.com/journals/information-systems/0306-4379/guide-for-authors}
\item
  compare with existing processes that are publicly documented

  \begin{itemize}
  \item
    AJPS Verification Policy:
    \url{https://ajps.org/ajps-verification-policy/}
  \item
    \ldots{}
  \end{itemize}
\item
  flexible level of detail? a check can be enhanced to (potentially
  semi-automatically) check for good practices

  \begin{itemize}
  \item
    ``A group of us in the ({\textbf{???}}) Software Citation
    Implementation Working Group produced a''Software Citation Checklist
    for Authors" \url{https://t.co/4Noe66FsoX} providing guidelines on
    how to cite software and describing why it is important and what
    software should be cited" via
    \url{https://twitter.com/alegonbel/status/1207664922932453376?s=09})
  \end{itemize}
\item
  focus on ECR as codecheckers

  \begin{itemize}
  \item
    ECR are the one's breaking new ground in schol comm, cf.~the
    percentages of career stages submitting registered reports on slide
    20 \textgreater{} \url{https://osf.io/2vjus/}
  \end{itemize}
\end{itemize}

\section*{Annotated Examples}\label{annotated-examples}

\begin{itemize}
\item
  \textbf{the default implementation}
\item
  critically discuss our/the default implementation vs.~the principles
\item
  HPC example
\item
  if we see the corpus of examles as a scientific dataset, it should be
  published in a citable way (maybe even submit to a data journal)
\end{itemize}

\section*{Open problems}\label{open-problems}

\begin{itemize}
\item
  Do publishers need to build up data science experts who can conduct
  checks?
\item
  Is an integration of software/data citation checks
  benefitial/realistic?
\item
  Bot-support and tools (linter, report authoring with R Markdown)
\item
  How can open platforms be used to enable a review in private
  (cf.~\url{https://www.cambridge.org/core/blog/2019/08/19/how-to-make-the-data-and-code-for-your-manuscript-available-to-peer-reviewers-before-making-it-public/}),
  if that is desired
\item
  Does nudging towards better practices work (highlight the successes)
  or is any check with opt-in doomed to fail?
\item
  Do we need more than one codechecker? Is there a similar diversity as
  in a peer review where more opinions are needed?
\item
  can CODECHECK work for preprints? how? is a re-check required?
\item
  Risk: is the hurdle too low? are ``levels'' of checks are solution to
  increase transparency of check's level of detail
\item
  CODECHECK in the context of innovations and disruptions in scientific
  publishing

  \begin{itemize}
  \item
    \url{https://doi.org/10.1042/ETLS20180172}
  \item
    \url{https://doi.org/10.3390/publications7020034}
  \end{itemize}
\item
  Should every code be checkd, or should the labour be capped by only
  checking ``important'' works or at important stages; if yes, isn't
  peer review such a stage?
\item
  see \url{https://github.com/codecheckers/discussion}
\item
  Can we establish code checks as a type of review within the schol comm
  metadata (or should we?) \textgreater{} we should not, put them on the
  same level as reviews!
\item
  How can we turn a CODECHECK into a positive experience?

  \begin{itemize}
  \item
    cf.~kindness in peer review in
    \url{https://www.mdpi.com/2304-6775/8/2/26}
  \end{itemize}
\item
  How can we allow reactions to checks? (commenting on reports)
\item
  Will bitwise reproducibility become easier than what a CODECHECK does?
  triggered by
  \url{https://twitter.com/khinsen/status/1242842759733665799?s=09})
\item
  If a report is not published for failures: Isn't there a bias for the
  codechecker to have a ``successful'' reproduction because then a
  report is published?
\end{itemize}

\end{document}
